{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "#Import packages\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from itertools import count\n",
    "from collections import deque\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import tqdm\n",
    "from tqdm import tnrange, tqdm_notebook\n",
    "from time import sleep\n",
    "import os\n",
    "\n",
    "#Import Custom Classes\n",
    "\n",
    "from Source.nn_model_dqn import QNetwork\n",
    "from Source.dqn_rcv_agent import Agent, ReplayBuffer, EpsilonGreedyStrategy\n",
    "from Source.env_manager import EnvManager\n",
    "from Source.misc_fun.utils import plot, get_moving_average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "#Hyper-parameters\n",
    "BUFFER_SIZE = int(1e5)      #replay buffer size\n",
    "BATCH_SIZE = 60             #minibatch size\n",
    "GAMMA = 0.999                #discount factor\n",
    "TAU = 1e-3                  #for soft update of target parameters\n",
    "LR = 5e-4                   #learning rate\n",
    "UPDATE_EVERY = 50            #how often to update the network\n",
    "eps_start = 1\n",
    "eps_end = 0.01\n",
    "eps_decay = 0.9983 #125e-6\n",
    "train_episodes = 25\n",
    "test_episodes = 1\n",
    "seed = 0                    #random seed number\n",
    "episode_step_limit = 50\n",
    "#%%\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QNetwork(\n",
      "  (hidden_layers): ModuleList(\n",
      "    (0): Linear(in_features=130, out_features=256, bias=True)\n",
      "    (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (2): Linear(in_features=256, out_features=64, bias=True)\n",
      "  )\n",
      "  (output): Linear(in_features=64, out_features=8, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Choose the environment\n",
    "em = EnvManager(device, 'combrf-v2', seed)\n",
    "available_actions = em.num_actions_available()\n",
    "random.seed(seed)\n",
    "state_size = em.state_size()\n",
    "\n",
    "#Select the strategy\n",
    "strategy = EpsilonGreedyStrategy(eps_start, eps_end, eps_decay)\n",
    "\n",
    "#Initialize the agent\n",
    "agent = Agent(strategy, state_size, available_actions, seed, device)\n",
    "\n",
    "#Instantiate MemoryBuffer\n",
    "memory = ReplayBuffer(available_actions, BUFFER_SIZE, BATCH_SIZE, seed, device)\n",
    "\n",
    "\n",
    "policy_net = QNetwork(state_size, available_actions, seed).to(device)\n",
    "target_net = QNetwork(state_size, available_actions, seed).to(device)\n",
    "print(policy_net)\n",
    "\n",
    "#Initialize target_net weights to policy_net weights\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval() #Set the target_net in eval mode\n",
    "\n",
    "#Select the optimizer\n",
    "optimizer = optim.Adam(params=policy_net.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## Test with random untrained actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4.72280004e-08  1.36425981e-07 -1.77550709e-07  3.31816633e-08\n",
      "   1.45853489e-07 -1.72510123e-07  1.89390689e-08  1.54418329e-07\n",
      "   1.36425981e-07 -1.77550709e-07  3.31816633e-08  1.45853489e-07\n",
      "  -1.72510123e-07  1.89390689e-08  1.54418329e-07 -1.66449203e-07\n",
      "  -1.77550709e-07  3.31816633e-08  1.45853489e-07 -1.72510123e-07\n",
      "   1.89390689e-08  1.54418329e-07 -1.66449203e-07  4.58445694e-09\n",
      "   3.31816633e-08  1.45853489e-07 -1.72510123e-07  1.89390689e-08\n",
      "   1.54418329e-07 -1.66449203e-07  4.58445694e-09  1.62069841e-07\n",
      "   1.45853489e-07 -1.72510123e-07  1.89390689e-08  1.54418329e-07\n",
      "  -1.66449203e-07  4.58445694e-09  1.62069841e-07 -1.59403799e-07\n",
      "  -1.72510123e-07  1.89390689e-08  1.54418329e-07 -1.66449203e-07\n",
      "   4.58445694e-09  1.62069841e-07 -1.59403799e-07 -9.79727037e-09\n",
      "   1.89390689e-08  1.54418329e-07 -1.66449203e-07  4.58445694e-09\n",
      "   1.62069841e-07 -1.59403799e-07 -9.79727037e-09  1.68762770e-07\n",
      "   1.54418329e-07 -1.66449203e-07  4.58445694e-09  1.62069841e-07\n",
      "  -1.59403799e-07 -9.79727037e-09  1.68762770e-07 -1.51415580e-07\n",
      "  -1.80959207e-07  1.27924472e-07  5.87576770e-08 -1.84053514e-07\n",
      "   1.17061857e-07  7.22286389e-08 -1.86059212e-07  1.05506865e-07\n",
      "   1.27924472e-07  5.87576770e-08 -1.84053514e-07  1.17061857e-07\n",
      "   7.22286389e-08 -1.86059212e-07  1.05506865e-07  8.52723952e-08\n",
      "   5.87576770e-08 -1.84053514e-07  1.17061857e-07  7.22286389e-08\n",
      "  -1.86059212e-07  1.05506865e-07  8.52723952e-08 -1.86964439e-07\n",
      "  -1.84053514e-07  1.17061857e-07  7.22286389e-08 -1.86059212e-07\n",
      "   1.05506865e-07  8.52723952e-08 -1.86964439e-07  9.33278384e-08\n",
      "   1.17061857e-07  7.22286389e-08 -1.86059212e-07  1.05506865e-07\n",
      "   8.52723952e-08 -1.86964439e-07  9.33278384e-08  9.78117969e-08\n",
      "   7.22286389e-08 -1.86059212e-07  1.05506865e-07  8.52723952e-08\n",
      "  -1.86964439e-07  9.33278384e-08  9.78117969e-08 -1.86763841e-07\n",
      "  -1.86059212e-07  1.05506865e-07  8.52723952e-08 -1.86964439e-07\n",
      "   9.33278384e-08  9.78117969e-08 -1.86763841e-07  8.05968125e-08\n",
      "   1.05506865e-07  8.52723952e-08 -1.86964439e-07  9.33278384e-08\n",
      "   9.78117969e-08 -1.86763841e-07  8.05968125e-08  1.09772678e-07\n",
      "  -1.02031746e-06 -6.19932212e-07]]\n",
      "0\n",
      "4\n",
      "7\n",
      "6\n",
      "4\n",
      "7\n",
      "5\n",
      "3\n",
      "Episode score: 6.971134851781217\n"
     ]
    }
   ],
   "source": [
    "obs = em.env.reset()\n",
    "print(obs)\n",
    "ep_rwd=[]\n",
    "while True:\n",
    "    action = random.randrange(em.env.action_space.n)\n",
    "    \n",
    "    obs, rwd, done, _ = em.env.step(action)\n",
    "    print(action)\n",
    "    ep_rwd.append(rwd)\n",
    "    if done:\n",
    "        break\n",
    "        \n",
    "print(\"Episode score: {}\".format(np.sum(ep_rwd)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## Train the DQN agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\pycharm_workspace\\precoding_rl\\venv\\lib\\site-packages\\ipykernel_launcher.py:4: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d489e95ee44b46d3835e52d94ac36b82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='training loop: ', max=25.0, style=ProgressStyle(descriptiâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Episode 1\t,\tScore: 6.95, eps: 0.9932173203563519, moving avg_rwd: 0.0\r",
      "\r",
      "Episode 2\t,\tScore: 6.64, eps: 0.9915288509117461, moving avg_rwd: 0.0\r",
      "\r",
      "Episode 3\t,\tScore: 6.80, eps: 0.9898432518651962, moving avg_rwd: 0.0\r",
      "\r",
      "Episode 4\t,\tScore: 6.87, eps: 0.9881605183370252, moving avg_rwd: 0.0\r",
      "\r",
      "Episode 5\t,\tScore: 6.68, eps: 0.9864806454558522, moving avg_rwd: 0.0\r",
      "\r",
      "Episode 6\t,\tScore: 6.93, eps: 0.9848036283585773, moving avg_rwd: 0.0\r"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Subtraction, the `-` operator, with a bool tensor is not supported. If you are trying to invert a mask, use the `~` or `logical_not()` operator instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-199ef75954d0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpolicy_net\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mnext_obs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_obs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m         \u001b[0mep_rwd\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[1;31m#memory.add(state, action, reward, next_state, done)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\pycharm_workspace\\precoding_rl\\Source\\dqn_rcv_agent.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, state, action, reward, next_state, done)\u001b[0m\n\u001b[0;32m     56\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m                 \u001b[0mexperiences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexperiences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mGAMMA\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\pycharm_workspace\\precoding_rl\\Source\\dqn_rcv_agent.py\u001b[0m in \u001b[0;36mlearn\u001b[1;34m(self, experiences, gamma)\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;31m#print(\"[Agent] Q_targets_next.shape: {}\".format(Q_targets_next.shape))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m         \u001b[1;31m#Compute best Q targets for the current policy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 112\u001b[1;33m         \u001b[0mQ_targets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrewards\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mgamma\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mQ_targets_next\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mdones\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    113\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m         \u001b[1;31m#Compute Q values for current states, actions pairs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\pycharm_workspace\\precoding_rl\\venv\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36m__rsub__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    401\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    402\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__rsub__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 403\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_VariableFunctions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrsub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    404\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    405\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__rdiv__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Subtraction, the `-` operator, with a bool tensor is not supported. If you are trying to invert a mask, use the `~` or `logical_not()` operator instead."
     ]
    }
   ],
   "source": [
    "ep_rewards = []\n",
    "policy_net.train()\n",
    "\n",
    "outer = tqdm.tqdm_notebook(total=train_episodes, desc='training loop: ', position=0)\n",
    "\n",
    "for episode in range(train_episodes):\n",
    "    obs = em.reset()\n",
    "\n",
    "    ep_loss = 0.0\n",
    "    ep_rwd = 0.0\n",
    "    timestep = 0\n",
    "    agent.current_step +=1\n",
    "    while True:\n",
    "        action = agent.act(obs, policy_net)\n",
    "        next_obs, reward, done, _ = em.step(action)\n",
    "        #agent.step(obs, action, reward, next_obs, done)\n",
    "        ep_rwd += reward.item()\n",
    "        memory.add(obs, action, reward, next_state, done)\n",
    "        \n",
    "        obs = next_obs\n",
    "        #state_tensor = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        \n",
    "        #if memory.can_provide_sample():\n",
    "        #    experiences = memory.sample()\n",
    "        #    states, actions, rewards, next_states, dones = experiences\n",
    "        #    \n",
    "        #    #print(states.shape, states.dtype)\n",
    "        #    #print(actions.unsqueeze(-1).shape)\n",
    "        #    current_q_values = policy_net(states).gather(1,index=actions.unsqueeze(-1))\n",
    "        #    next_q_values = target_net(next_states).detach().max(1)[0]\n",
    "        #    target_q_values = (next_q_values*GAMMA) + rewards\n",
    "            \n",
    "        #    loss = F.mse_loss(current_q_values, target_q_values.unsqueeze(1))\n",
    "            #print(\"loss: \", loss.item())\n",
    "        #    ep_loss += loss.item()\n",
    "            \n",
    "        #    optimizer.zero_grad()\n",
    "        #    loss.backward()\n",
    "        #    optimizer.step()\n",
    "        \n",
    "        if done:\n",
    "            ep_rewards.append(ep_rwd)\n",
    "            moving_avg_rwd = get_moving_average(100, ep_rewards)\n",
    "            print('\\rEpisode {}\\t,\\tScore: {:.2f}, eps: {}, moving avg_rwd: {}'.format(episode+1, ep_rwd, agent.strategy.get_exploration_rate(agent.current_step), moving_avg_rwd[-1]), end=\"\\r\")\n",
    "            #plot(episode_rewards, 100)\n",
    "            break\n",
    "            \n",
    "        \n",
    "    if episode % UPDATE_EVERY == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "    \n",
    "    if (np.mean(ep_rewards[-100:]) >= 20000):\n",
    "        print(\"Goal is reached in {} episodes!\\n\".format(episode))\n",
    "        break\n",
    "        \n",
    "    \n",
    "    \n",
    "    # update tqdm bar\n",
    "    outer.update(1)\n",
    "    \n",
    "torch.save(policy_net.state_dict(), 'checkpoint.pth')    \n",
    "#timer.finish()\n",
    "plot(episode_rewards, 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
