{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "#Import packages\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from itertools import count\n",
    "from collections import deque\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import tqdm\n",
    "from tqdm import tnrange, tqdm_notebook\n",
    "from time import sleep\n",
    "import os\n",
    "\n",
    "#Import Custom Classes\n",
    "\n",
    "from Source.nn_model_dqn import QNetwork\n",
    "from Source.dqn_rcv_agent import Agent, ReplayBuffer, EpsilonGreedyStrategy\n",
    "from Source.env_manager import EnvManager\n",
    "from Source.misc_fun.utils import plot, get_moving_average, Generate_BeamDir, All_Exhaustive_RateMeas\n",
    "from Source.PER import PrioritizedReplayBuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "#Hyper-parameters\n",
    "BUFFER_SIZE = int(1e5)      #replay buffer size\n",
    "BATCH_SIZE = 128             #minibatch size\n",
    "GAMMA = 0.999                #discount factor\n",
    "ALPHA = 1.0                  #non-stationarity parameter\n",
    "TAU = 1e-3                  #for soft update of target parameters\n",
    "LR = 5e-4                   #learning rate\n",
    "TEST_EVERY = 1600            #how often to test the network\n",
    "eps_start = 1.0\n",
    "eps_end = 0.01\n",
    "eps_decay = 0.9987 #125e-6\n",
    "PER_ALPHA = 0.6\n",
    "PER_BETA = 0.4\n",
    "PRIORITIZED_REPLAY = False\n",
    "\n",
    "episodes = 4200 #3100 train, 500 test\n",
    "seed = 0                    #random seed number\n",
    "#%%\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "inp_fptr = open(\"ricianch_variation.txt\")\n",
    "ch_randvals = inp_fptr.read().splitlines()\n",
    "ch_randvals = [np.complex(a.replace('i','j')) for a in ch_randvals]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1.1128+0.0082264j)\n"
     ]
    }
   ],
   "source": [
    "print(ch_randvals[0])\n",
    "\n",
    "#print(np.complex(ch_randvals[0].replace('i','j')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beamset: [22.35, 33.8, 11.46, 28.07, 16.62, 45.26, 56.15, 50.42, 39.53, 67.61, 78.5, 73.34, 61.88, 89.95, 101.41, 95.68, 84.22, 112.3, 123.76, 118.03, 107.14, 135.22, 146.1, 140.37, 129.49, 157.56, 169.02, 163.29, 151.83, 179.91, 5.73, 174.18]\n",
      "32\n",
      "[0.39 0.59 0.2  0.49 0.29 0.79 0.98 0.88 0.69 1.18 1.37 1.28 1.08 1.57\n",
      " 1.77 1.67 1.47 1.96 2.16 2.06 1.87 2.36 2.55 2.45 2.26 2.75 2.95 2.85\n",
      " 2.65 3.14 0.1  3.04]\n",
      "Ntx: 8, Beam: [ 0.35355339+0.j         -0.35350985+0.00554875j  0.35337922-0.01109613j\n",
      " -0.35316156+0.01664078j  0.3528569 -0.02218133j -0.35246532+0.02771642j\n",
      "  0.35198692-0.03324468j -0.35142182+0.03876475j]\n",
      "Ntx: 8, Beam: [ 0.35355339+0.00000000e+00j -0.35355339-1.40869652e-06j\n",
      "  0.35355339+2.81739304e-06j -0.35355339-4.22608956e-06j\n",
      "  0.35355339+5.63478608e-06j -0.35355339-7.04348259e-06j\n",
      "  0.35355339+8.45217911e-06j -0.35355339-9.86087563e-06j]\n"
     ]
    }
   ],
   "source": [
    "#Testing (beam_dir, beam_width) Beamset\n",
    "from Source.antenna.ula import steervec\n",
    "from Source.misc_fun.utils import Generate_BeamDirs, plotbeam\n",
    "\n",
    "refinelevels = np.array([0,1,2])\n",
    "beamset = Generate_BeamDirs(8,refinelevels)\n",
    "\n",
    "print(\"Beamset: {}\".format([np.round(x*180/np.pi, decimals=2) for x in beamset]))\n",
    "print(len(beamset))\n",
    "print(beamset)\n",
    "tx_beam = steervec(8,beamset[-2], 0)\n",
    "print(\"Ntx: {}, Beam: {}\".format(8, tx_beam))\n",
    "\n",
    "tx_beam = steervec(8,beamset[-3], 0)\n",
    "print(\"Ntx: {}, Beam: {}\".format(8, tx_beam))\n",
    "\n",
    "#theta, gr = plotbeam(beamset[-2], 8)\n",
    "#theta2, gr2 = plotbeam(beamset[2], 8)\n",
    "#theta3, gr3 = plotbeam(beamset[0], 8)\n",
    "#ax = plt.subplot(111, projection='polar')\n",
    "##print(theta.shape, gr.shape)\n",
    "#ax.plot(theta, gr, theta2, gr2, theta3, gr3)\n",
    "#plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {},
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uma-nlos\n",
      "[[-17.36481777  49.24038765  22.5       ]\n",
      " [-50.         -29.60405375  22.5       ]\n",
      " [ 17.10100717  93.96926208  22.5       ]\n",
      " [  4.35778714  98.26285905  22.5       ]]\n",
      "QNetwork(\n",
      "  (hidden_layers): ModuleList(\n",
      "    (0): Linear(in_features=5, out_features=400, bias=True)\n",
      "    (1): Linear(in_features=400, out_features=400, bias=True)\n",
      "    (2): Linear(in_features=400, out_features=64, bias=True)\n",
      "  )\n",
      "  (softmax): Softmax(dim=1)\n",
      "  (output): Linear(in_features=64, out_features=8, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Choose the environment\n",
    "em = EnvManager(device, 'combrf-v8', seed)\n",
    "available_actions = em.num_actions_available()\n",
    "random.seed(seed)\n",
    "state_size = em.state_size()\n",
    "print(em.env.ch_model)\n",
    "print(em.env.sc_xyz)\n",
    "#Select the strategy\n",
    "strategy = EpsilonGreedyStrategy(eps_start, eps_end, eps_decay, 3500)\n",
    "\n",
    "if PRIORITIZED_REPLAY:\n",
    "    beta_strategy = EpsilonGreedyStrategy(PER_BETA, 1.0, eps_decay, 3500)\n",
    "\n",
    "#Initialize the agent\n",
    "agent = Agent(strategy, state_size, available_actions, seed, device)\n",
    "\n",
    "#Instantiate MemoryBuffer\n",
    "if not PRIORITIZED_REPLAY:\n",
    "    memory = ReplayBuffer(available_actions, BUFFER_SIZE, BATCH_SIZE, seed, device)\n",
    "else:\n",
    "    memory = PrioritizedReplayBuffer(available_actions, BUFFER_SIZE, BATCH_SIZE, PER_ALPHA,seed, device)\n",
    "\n",
    "policy_net = QNetwork(state_size, available_actions, seed).to(device)\n",
    "target_net = QNetwork(state_size, available_actions, seed).to(device)\n",
    "print(policy_net)\n",
    "\n",
    "#Initialize target_net weights to policy_net weights\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval() #Set the target_net in eval mode\n",
    "\n",
    "#Select the optimizer\n",
    "optimizer = optim.Adam(params=policy_net.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tx_locs: [array([[93.96926208, 34.20201433, 25.        ]]), array([[93.96926208, 64.27876097, 25.        ]]), array([[93.96926208, 86.60254038, 25.        ]]), array([[93.96926208, 98.4807753 , 25.        ]]), array([[76.60444431, 34.20201433, 25.        ]]), array([[76.60444431, 64.27876097, 25.        ]]), array([[76.60444431, 86.60254038, 25.        ]]), array([[76.60444431, 98.4807753 , 25.        ]]), array([[50.        , 34.20201433, 25.        ]]), array([[50.        , 64.27876097, 25.        ]]), array([[50.        , 86.60254038, 25.        ]]), array([[50.       , 98.4807753, 25.       ]]), array([[17.36481777, 34.20201433, 25.        ]]), array([[17.36481777, 64.27876097, 25.        ]]), array([[17.36481777, 86.60254038, 25.        ]]), array([[17.36481777, 98.4807753 , 25.        ]])]\n",
      "Total locations:  16\n"
     ]
    }
   ],
   "source": [
    "print(\"tx_locs: {}\".format(em.env.tx_locs))\n",
    "print(\"Total locations: \", len(em.env.tx_locs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## Test with random untrained actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.5        0.         0.5        0.34202014 1.        ]]\n",
      "6\n",
      "6\n",
      "Episode score: -1.0\n"
     ]
    }
   ],
   "source": [
    "obs = em.env.reset(np.exp(1j * 2 * np.pi * 0.6))\n",
    "#print(len(em.env.beamwidth_vec))\n",
    "#print(em.env.action_space.n)\n",
    "print(obs)\n",
    "ep_rwd=[]\n",
    "while True:\n",
    "    action = random.randrange(em.env.action_space.n)\n",
    "    \n",
    "    obs, rwd, done, _ = em.env.step(action)\n",
    "    print(action)\n",
    "    ep_rwd.append(rwd)\n",
    "    if done:\n",
    "        break\n",
    "        \n",
    "print(\"Episode score: {}\".format(np.sum(ep_rwd)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After adding moving patterns:  [[array([[-17.36481777,  49.24038765,  22.5       ]]), array([[82.63518223, 49.24038765, 22.5       ]]), array([[182.63518223,  49.24038765,  22.5       ]])], [array([[-50.        , -29.60405375,  22.5       ]]), array([[ 50.        , -29.60405375,  22.5       ]]), array([[150.        , -29.60405375,  22.5       ]])], [array([[17.10100717, 93.96926208, 22.5       ]]), array([[17.10100717, -6.03073792, 22.5       ]]), array([[  17.10100717, -106.03073792,   22.5       ]])], [array([[ 4.35778714, 98.26285905, 22.5       ]]), array([[ 4.35778714, -1.73714095, 22.5       ]]), array([[   4.35778714, -101.73714095,   22.5       ]])]]\n"
     ]
    }
   ],
   "source": [
    "moving_rpts = [[np.array([rp_loc])] for rp_loc in em.env.sc_xyz]\n",
    "\n",
    "#print(\"Initial: \\n\", moving_rpts)\n",
    "\n",
    "for i in range(2):\n",
    "    for ndx in range(len(moving_rpts)):\n",
    "        if (ndx == 0) or (ndx == 1):\n",
    "            last_loc = moving_rpts[ndx][-1]\n",
    "            new_loc = np.array([[last_loc[0][0]+em.env.rx_stepsize, last_loc[0][1], last_loc[0][2]]]) \n",
    "            moving_rpts[ndx].append(new_loc)\n",
    "        elif (ndx == 2) or (ndx == 3):\n",
    "            last_loc = moving_rpts[ndx][-1]\n",
    "            new_loc = np.array([[last_loc[0][0], last_loc[0][1]-em.env.rx_stepsize, last_loc[0][2]]]) \n",
    "            moving_rpts[ndx].append(new_loc)\n",
    "\n",
    "print(\"After adding moving patterns: \", moving_rpts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "eps_scpts = []\n",
    "\n",
    "for i in range(episodes):\n",
    "    #print(random.choice(moving_rpts[0]))\n",
    "    sc_pts = np.array([random.choice(moving_rpts[ndx])[0] for ndx in range(len(moving_rpts))])\n",
    "    eps_scpts.append(sc_pts)\n",
    "\n",
    "#print(eps_scpts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[182.63518223  49.24038765  22.5       ]\n",
      " [-50.         -29.60405375  22.5       ]\n",
      " [ 17.10100717  -6.03073792  22.5       ]\n",
      " [  4.35778714  -1.73714095  22.5       ]]\n"
     ]
    }
   ],
   "source": [
    "print(eps_scpts[7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## Train the DQN agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c958bc1f217491bbe87088b9344db15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='training loop: ', max=4200.0, style=ProgressStyle(descripâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 4099,\tScore: 1.00, eps: 0.01, moving avg_rwd: 0.8799999952316284, ep_loss: 123.2599487304687525251312255859488\r"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'int' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-3c7af1ef1176>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepisode\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m4100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[1;31m#obs = em.test_reset(np.array([[100*np.cos(em.env.deg),100*np.sin(em.env.deg),25.0]]), test_txbdir, em.env.sc_xyz, ch_randvals[episode])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m         \u001b[0mobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtx_locs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest_loc_ndx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msc_xyz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mch_randvals\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mepisode\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m         \u001b[0mtest_loc_ndx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtest_loc_ndx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtx_locs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;31m#test_txbdir = (test_txbdir + 1) % em.env.obs_space.nvec[3]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\pycharm_workspace\\precoding_rl\\Source\\env_manager.py\u001b[0m in \u001b[0;36mtest_reset\u001b[1;34m(self, tx_loc, sc, ch_randval, tbdir_ndx)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtest_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx_loc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mch_randval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtbdir_ndx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobs\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtx_loc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtbdir_ndx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mch_randval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#.unsqueeze(0)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\pycharm_workspace\\precoding_rl\\gym_combrf\\envs\\CombRF_Env_v8.py\u001b[0m in \u001b[0;36mtest_reset\u001b[1;34m(self, tx_loc, tbdir_ndx, sc, ch_randval)\u001b[0m\n\u001b[0;32m    252\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtx_num\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_txloc_ndx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtx_loc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    253\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchannel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate_paths\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mch_randval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtx_num\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 254\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdqnobs_counter\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtx_num\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobs_space\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnvec\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtx_dir_ndx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnpaths\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchannel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlos_path\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchannel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_h\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#channel coefficient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'int' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "ep_rewards = []\n",
    "test_rewards = []\n",
    "test_data_rates = []\n",
    "test_eps_iters=[]\n",
    "test_minexh_rates = []\n",
    "test_maxexh_rates = []\n",
    "false_positives =0\n",
    "policy_net.train()\n",
    "test_txbdir = 0\n",
    "\n",
    "strategy = EpsilonGreedyStrategy(eps_start, eps_end, eps_decay, 4100)\n",
    "outer = tqdm.notebook.tqdm(total=episodes, desc='training loop: ', position=0)\n",
    "train_steps = 0\n",
    "test_loc_ndx = 0\n",
    "for episode in range(episodes):\n",
    "    \n",
    "    #if(eps <= 0.1):\n",
    "    #    em.env.sc_xyz=np.array([])\n",
    "    #    em.env.ch_model = 'uma-los'\n",
    "    #if ((episode+1) == 3500):\n",
    "    #    em.env.ch_model = 'uma-los'\n",
    "    #    em.env.sc_xyz = np.array([])\n",
    "        #train_steps = 0\n",
    "        #strategy =EpsilonGreedyStrategy(1.0, eps_end, 0.997)#0.1\n",
    "    #if ((episode+1) == 3900):\n",
    "    #    train_steps=0\n",
    "    #    strategy =EpsilonGreedyStrategy(1.0, eps_end, 0.9983, 3500)#0.1\n",
    "    #    if PRIORITIZED_REPLAY:\n",
    "    #        beta_strategy = EpsilonGreedyStrategy(PER_BETA, 1.0, eps_decay, 3500)\n",
    "    #em.env.sc_xyz = eps_scpts[episode]\n",
    "    #if (episode+1) == 4100:\n",
    "    #    obs_tensor = torch.tensor(np.array([[-100,-100,21.5]]), device=device, dtype=torch.float32)\n",
    "    #    print(\"TXloc: {}, obs_tensor: {}\".format(np.array([[-100,-100,21.5]]), obs_tensor))\n",
    "    #    for action in range(em.env.action_space.n):\n",
    "    #        action_tensor = torch.tensor([action]).to(device)\n",
    "    #        learnt_qval = policy_net(obs_tensor).gather(1,index=action_tensor)\n",
    "    #        print(\"action: {}, learnt_qval: {}\".format(action, learnt_qval))\n",
    "        \n",
    "    if ((episode+1) >= 4100):    \n",
    "        #obs = em.test_reset(np.array([[100*np.cos(em.env.deg),100*np.sin(em.env.deg),25.0]]), test_txbdir, em.env.sc_xyz, ch_randvals[episode])\n",
    "        obs = em.test_reset(em.env.tx_locs[test_loc_ndx], test_txbdir, em.env.sc_xyz, ch_randvals[episode])\n",
    "        test_loc_ndx = (test_loc_ndx + 1) % len(em.env.tx_locs)\n",
    "        #test_txbdir = (test_txbdir + 1) % em.env.obs_space.nvec[3]\n",
    "    else:\n",
    "        obs = em.reset(ch_randvals[episode])\n",
    "\n",
    "    \n",
    "    ep_loss = 0.0\n",
    "    ep_rwd = 0.0\n",
    "    timestep = 0\n",
    "    tx_dirs = []\n",
    "    rx_dirs = []\n",
    "    data_rates =[]\n",
    "    #agent.current_step +=1\n",
    "    train_steps +=1\n",
    "    eps = strategy.get_exploration_rate(train_steps)\n",
    "    if PRIORITIZED_REPLAY:\n",
    "        beta = beta_strategy.get_exploration_rate(train_steps)\n",
    "    \n",
    "    while True:\n",
    "        tx_dirs.append(em.env.tx_bdir*(180/np.pi))\n",
    "        rx_dirs.append(em.env.rx_bdir*(180/np.pi))\n",
    "        data_rates.append(em.env.rate)\n",
    "\n",
    "        action = agent.act(obs, policy_net, eps)\n",
    "        next_obs, reward, done, _ = em.step(action)\n",
    "        #agent.step(obs, action, reward, next_obs, done)\n",
    "        ep_rwd += reward.item()\n",
    "        \n",
    "        min_exh_rate, max_exh_rate,min_action_ndx,max_action_ndx,_,_ = em.env.get_minmax_exhrate(ch_randvals[episode])\n",
    "        '''\n",
    "        if(np.all(em.env.tx_loc == np.array([[100,100,0]]))):\n",
    "            print(\"ch_model: {}, sc_xyz: {}, min_exh_rate: {}, max_exh_rate: {}, min_action_ndx: {}, max_action_ndx: {}\".format(em.env.ch_model, em.env.sc_xyz, min_exh_rate, max_exh_rate, min_action_ndx, max_action_ndx))\n",
    "        '''   \n",
    "        memory.add(obs, action, reward, next_obs, done)\n",
    "        obs = next_obs\n",
    "        \n",
    "        if memory.can_provide_sample():\n",
    "            \n",
    "            if PRIORITIZED_REPLAY:\n",
    "                experiences = memory.sample(beta)\n",
    "                observations, actions, rewards, next_observations, dones, weights, batch_indices = experiences\n",
    "            else:\n",
    "                experiences = memory.sample()\n",
    "                observations, actions, rewards, next_observations, dones = experiences\n",
    "                weights, batch_indices = torch.tensor(np.ones_like(rewards.cpu().data.numpy())).to(device), None\n",
    "            \n",
    "            #print(states.shape, states.dtype)\n",
    "            #print(actions.unsqueeze(-1).shape)\n",
    "            current_q_values = policy_net(observations).gather(1,index=actions.unsqueeze(-1))#(1-ALPHA)*\n",
    "            \n",
    "            next_q_values = target_net(next_observations).detach().max(1)[0]\n",
    "            target_q_values = ((next_q_values*GAMMA) + rewards)\n",
    "            #print(policy_net(observations).type())\n",
    "            #print(target_q_values.unsqueeze(1).type())\n",
    "            #loss = F.mse_loss(current_q_values, target_q_values.unsqueeze(1).float())\n",
    "            td_errors = ALPHA*(target_q_values.unsqueeze(1).float()-current_q_values)\n",
    "            loss = F.mse_loss(torch.zeros(current_q_values.size()).to(device), ALPHA*(target_q_values.unsqueeze(1).float()-current_q_values),  reduction=None)\n",
    "            #weighted_loss = torch.mean(weights*loss)\n",
    "            #print(\"loss: \", loss)\n",
    "            #loss = ALPHA*loss\n",
    "            #ep_loss += weighted_loss.item()\n",
    "            ep_loss += loss.item()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            #weighted_loss.backward()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if PRIORITIZED_REPLAY:\n",
    "                comp_errors = np.array([x[0] for x in td_errors.cpu().data.numpy()])\n",
    "                new_priorities = np.abs(comp_errors) + 1e-6\n",
    "                #print(new_priorities)\n",
    "                memory.update_priorities(batch_indices.cpu().data.numpy(), new_priorities)\n",
    "            \n",
    "        timestep +=1\n",
    "        if done:\n",
    "            ep_rewards.append(ep_rwd)\n",
    "            moving_avg_rwd = get_moving_average(100, ep_rewards)\n",
    "            print('\\rEpisode {},\\tScore: {:.2f}, eps: {}, moving avg_rwd: {}, ep_loss: {}'.format(episode+1, ep_rwd, eps, moving_avg_rwd[-1], ep_loss), end=\"\\r\")\n",
    "            #plot(episode_rewards, 100)\n",
    "            \n",
    "            #if(((episode+1) >= 6000) or (3000 <= (episode+1)<=3500)):\n",
    "            if((episode+1) >= 4100):\n",
    "                \n",
    "                test_data_rates.append(data_rates[-1])\n",
    "                test_eps_iters.append(timestep)\n",
    "                test_minexh_rates.append(min_exh_rate)\n",
    "                test_maxexh_rates.append(max_exh_rate)\n",
    "                \n",
    "                #if (timestep > 3):\n",
    "                #    if(timestep >=8):\n",
    "                #        false_positives +=1\n",
    "                #if (min_exh_rate == max_exh_rate):\n",
    "                print(\"\\ntest_eps: {0}, eps_rwd: {1}\".format(episode+1, ep_rwd))\n",
    "                print(\"TX loc: {}\".format(em.env.tx_loc))\n",
    "                print(\"SC_xyz: {}\".format(em.env.sc_xyz))\n",
    "                print(\"TX dirs: \", tx_dirs)\n",
    "                print(\"RX dirs: \", rx_dirs)\n",
    "                print(\"data rates: {}\".format(data_rates))\n",
    "                print(\"min exh_rate: {}, min_action_ndx: {}, beamset val: {}\".format(min_exh_rate, min_action_ndx, em.env.BeamSet[min_action_ndx]*180/np.pi))\n",
    "                print(\"max exh_rate: {}, max_action_ndx: {}, beamset val: {}\\n\".format(max_exh_rate, max_action_ndx, em.env.BeamSet[max_action_ndx]*180/np.pi))\n",
    "                \n",
    "            timestep = 0\n",
    "            break\n",
    "            \n",
    "        #print(\"timestep: {}\".format(timestep))\n",
    "    #if episode % UPDATE_EVERY == 0:\n",
    "    #    target_net.load_state_dict(policy_net.state_dict())\n",
    "    for local_param, target_param in zip(policy_net.parameters(), target_net.parameters()):\n",
    "        target_param.data.copy_(TAU*local_param.data + (1.0-TAU)*target_param.data)\n",
    "    \n",
    "    \n",
    "    '''    \n",
    "    if ((episode == 0) or (episode+1) == 3100) or (((episode+1) > 3100) and ((train_steps) % (TEST_EVERY) == 0)):\n",
    "        #test some episodes to check the performance\n",
    "        policy_net.eval()\n",
    "        test_ep_rwds = []\n",
    "        for test_eps in range(test_episodes):\n",
    "            obs = em.test_reset(test_loc, em.env.action_space.sample(), em.env.sc_xyz, ch_randvals[episode+test_eps+1])\n",
    "            test_score=0\n",
    "            tx_dirs = []\n",
    "            rx_dirs = []\n",
    "            data_rates =[]\n",
    "            step = 0\n",
    "            done = False\n",
    "            while not done:\n",
    "                action = policy_net(obs).argmax(dim=1).to(device)#agent.act(state_tensor, policy_net)\n",
    "                #print(\"timestep: \",step, policy_net(obs),policy_net(obs).size(), action)\n",
    "                tx_dirs.append(em.env.tx_bdir*(180/np.pi))\n",
    "                rx_dirs.append(em.env.rx_bdir*(180/np.pi))\n",
    "                data_rates.append(em.env.rate)\n",
    "                next_obs, reward, done, _ = em.step(action)\n",
    "                step+=1\n",
    "                print(\"\\ntstep: {0}, obs: {1}, action: {2}, next_obs: {3}, rwd: {4}, done: {5}\".format(step, obs.cpu().data.numpy(), action.cpu().data.numpy(), next_obs.cpu().data.numpy(), reward.item(), done.item()))\n",
    "                test_score+=reward.item()\n",
    "                obs = next_obs\n",
    "            \n",
    "            print(\"test_eps: {0}, eps_rwd: {1}\".format(test_eps+1, test_score))\n",
    "            print(\"TX loc: {}\".format(em.env.tx_loc))\n",
    "            print(\"TX dirs: \", tx_dirs)\n",
    "            print(\"RX dirs: \", rx_dirs)\n",
    "            print(\"data rates: {}\".format(data_rates))\n",
    "            min_exh_rate, max_exh_rate,min_action_ndx,max_action_ndx,_,_ = em.env.get_minmax_exhrate(ch_randvals[episode+test_eps+1])\n",
    "            print(\"min exh_rate: {}, min_action_ndx: {}\".format(min_exh_rate, min_action_ndx))\n",
    "            print(\"max exh_rate: {}, max_action_ndx: {}\".format(max_exh_rate, max_action_ndx))\n",
    "            \n",
    "            test_ep_rwds.append(test_score)\n",
    "            test_rewards.append(test_score)\n",
    "            test_data_rates.append(data_rates[-1])\n",
    "            test_minexh_rates.append(min_exh_rate)\n",
    "            test_maxexh_rates.append(max_exh_rate)\n",
    "            \n",
    "        print(\"Average test_ep_score: {}\\n\".format(np.mean(test_ep_rwds)))\n",
    "        policy_net.train()\n",
    "        #agent.current_step=0\n",
    "        train_steps = 0\n",
    "        strategy =EpsilonGreedyStrategy(0.5, eps_end, 0.997)#0.1\n",
    "    else:\n",
    "        test_rewards.append(0)\n",
    "        test_data_rates.append(0)\n",
    "        test_minexh_rates.append(0)\n",
    "        test_maxexh_rates.append(0)\n",
    "    '''\n",
    "    # update tqdm bar\n",
    "    outer.update(1)\n",
    "    \n",
    "torch.save(policy_net.state_dict(), 'checkpoint.pth')\n",
    "\n",
    "#timer.finish()\n",
    "plot(ep_rewards, 200, test_rewards)\n",
    "print(\"No. of false positives: {}\".format(false_positives))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "fig=plt.figure()\n",
    "min_ndx=0\n",
    "max_ndx=500\n",
    "plt.plot(test_eps_iters[min_ndx:max_ndx])\n",
    "plt.show()\n",
    "print(np.mean(test_eps_iters), len(test_eps_iters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "print(em.env.ch_model)\n",
    "print(em.env.sc_xyz)\n",
    "print(np.mean(test_data_rates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=[9, 6])\n",
    "min_ndx=24\n",
    "max_ndx=48\n",
    "plt.plot(np.arange(len(test_data_rates[min_ndx:max_ndx])), test_data_rates[min_ndx:max_ndx], 'b', np.arange(len(test_data_rates[min_ndx:max_ndx])), test_minexh_rates[min_ndx:max_ndx], 'r--', np.arange(len(test_data_rates[min_ndx:max_ndx])), test_maxexh_rates[min_ndx:max_ndx], 'g--')\n",
    "#plt.plot(np.arange(len(test_data_rates[3080:3140])), test_data_rates[3080:3140], 'b', np.arange(len(test_data_rates[3080:3140])), test_minexh_rates[3080:3140], 'r--', np.arange(len(test_data_rates[3080:3140])), test_maxexh_rates[3080:3140],'g--')\n",
    "#plt.plot(np.arange(len(test_data_rates[4650:4740])), test_data_rates[4650:4740], 'b', np.arange(len(test_data_rates[4650:4740])), test_minexh_rates[4650:4740], 'r--', np.arange(len(test_data_rates[4650:4740])), test_maxexh_rates[4650:4740],'g--')\n",
    "#plt.plot(np.arange(len(test_data_rates[6230:])), test_data_rates[6230:], 'b', np.arange(len(test_data_rates[6230:])), test_minexh_rates[6230:], 'r--', np.arange(len(test_data_rates[6230:])), test_maxexh_rates[6230:],'g--')\n",
    "\n",
    "plt.xticks(np.arange(0, max_ndx-min_ndx), [str(x) for x in np.arange(min_ndx, max_ndx)])\n",
    "plt.legend(['learnt rate','min exhrate', 'max exhrate'])\n",
    "plt.xlabel('Episode #')\n",
    "plt.ylabel('data rate (bits/s)')\n",
    "plt.title('Beam alignment with episode length:{}'.format(em.env.goal_steps))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "min_ndx=60\n",
    "max_ndx=80\n",
    "print(test_data_rates[min_ndx:max_ndx])\n",
    "print(test_maxexh_rates[min_ndx:max_ndx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "print(test_data_rates[69])\n",
    "print([x*180/np.pi for x in em.env.BeamSet])\n",
    "#print(em.env.BeamSet[9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(em.env.action_space.n)\n",
    "print(em.env.BeamSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "print(em.env.dqnobs_counter)\n",
    "print(len(em.env.dqnobs_counter))\n",
    "print(np.mean(em.env.dqnobs_counter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "#import pickle\n",
    "\n",
    "#memory.save('memory_checkpoint.pth')\n",
    "plot(ep_rewards, 100, test_rewards)\n",
    "print(ep_rewards[:100])\n",
    "moving_avg_rwd = get_moving_average(100, ep_rewards[:100])\n",
    "print(moving_avg_rwd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## Test the DQN agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "from Source.misc_fun.utils import var_plotbeam\n",
    "\n",
    "print(tx_dirs)\n",
    "print(rx_dirs)\n",
    "for tx_ang, rx_ang in zip(tx_dirs, rx_dirs):\n",
    "    tx_theta, tx_gr = var_plotbeam(tx_ang, em.env.N_tx)\n",
    "    rx_theta, rx_gr = var_plotbeam(rx_ang, em.env.N_rx)\n",
    "    ax1 = plt.subplot(122, projection='polar')\n",
    "    ax1.plot(tx_theta, tx_gr)\n",
    "\n",
    "    ax2 = plt.subplot(121, projection='polar')\n",
    "    ax2.plot(rx_theta, rx_gr)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## Display all exhaustive rate measurements from env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D  # noqa: F401 unused import\n",
    "from matplotlib.collections import PolyCollection\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors as mcolors\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.gca(projection='3d')\n",
    "poly = PolyCollection(verts[:4], facecolors=['r', 'g', 'b', 'y'], alpha=0.6)\n",
    "print(len(verts))\n",
    "print(verts[0:2])\n",
    "#print(np.array(tx_locs[:4]))\n",
    "ax.add_collection3d(poly, zs=np.arange(1,5), zdir='y')\n",
    "\n",
    "ax.set_xlabel('X (beam pairs)')\n",
    "ax.set_ylabel('Y (tx_locs)')\n",
    "ax.set_zlabel('Z (data rates)')\n",
    "ax.set_xlim(0, 10)\n",
    "ax.set_ylim(1, 5)\n",
    "ax.set_zlim(10, 30)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "print(em.env.ch_model)\n",
    "print(em.env.sc_xyz)\n",
    "em.env.BeamSet[5][0]*180/np.pi\n",
    "np.arctan(1100/200)*180/np.pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "12/24, 14/24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "from Source.misc_fun.utils import plotbeam\n",
    "\n",
    "tx_theta, tx_gr = plotbeam(exh_txbeams[0]*(np.pi/180), em.env.N_tx)\n",
    "rx_theta, rx_gr = plotbeam(exh_rxbeams[0]*(np.pi/180), em.env.N_rx)\n",
    "ax1 = plt.subplot(122, projection='polar')\n",
    "ax1.plot(tx_theta, tx_gr)\n",
    "\n",
    "ax2 = plt.subplot(121, projection='polar')\n",
    "ax2.plot(rx_theta, rx_gr)\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
